{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd71a75",
   "metadata": {},
   "source": [
    "# How do Transformers work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c1e43",
   "metadata": {},
   "source": [
    "All transformers are trained as language models using large amounts of raw text through self-supervised learning.\n",
    "\n",
    "Self-supervised learning is a training method where the model's objective is automatically computed from its inputs, eliminating the need for labeled data.\n",
    "\n",
    "While this approach helps the model develop a statistical understanding of language, it's less practical for specific tasks. That's why pretrained models typically undergo fine-tuning(a supervised training process using human-annotated label)to adapt them for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fe138",
   "metadata": {},
   "source": [
    "## Transformers are big models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26bf84",
   "metadata": {},
   "source": [
    "The general strategiy to achieve good performances in transformers is increasing the models size as well as the amount of data they are pretrained on:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486a598",
   "metadata": {},
   "source": [
    "![image.png](images/image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f22b7",
   "metadata": {},
   "source": [
    "Training large models requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fedf3",
   "metadata": {},
   "source": [
    "![image.png](images/image%201.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53077cac",
   "metadata": {},
   "source": [
    "So, sharing language models is a crucial strategy in order to reduce overall compute cost and carbon footprint, and also to allow developers to work on top of already trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a226a",
   "metadata": {},
   "source": [
    "## Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7822f5e",
   "metadata": {},
   "source": [
    "Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3629db5",
   "metadata": {},
   "source": [
    "![image.png](images/image%202.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81093f",
   "metadata": {},
   "source": [
    "The pretraining is done on very large amounts of data., it requires a very large corpus of data and training can take up to several weeks.\n",
    "\n",
    "Fine-tuning is the training process that occurs after a model has been pretrained. It builds upon an existing pretrained model, then perform additional training with a dataset specific to your task. The fine-tuning process it requires way less data(in comparison to pretraining) to get decent results, so the amount of time and resources needed are much lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefbf7a",
   "metadata": {},
   "source": [
    "![image.png](images/image%203.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee5578",
   "metadata": {},
   "source": [
    "Then, it’s usually a good practice to try to leverage a pretrained model(as close as possible to the task you have to do) and fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40726331",
   "metadata": {},
   "source": [
    "## General Transformer Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93319e",
   "metadata": {},
   "source": [
    "The model is composed of two blocks:\n",
    "\n",
    "- **Encoder:** Receives an input and builds a representation of it(its features)\n",
    "- **Decoder:** Use the encoder’s representation along with other inputs to generate the tarquet sequence.\n",
    "\n",
    "So, in training the model is optimized to acquire understanding from input and generating outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1441d0",
   "metadata": {},
   "source": [
    "![image.png](images/image%204.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5717cac",
   "metadata": {},
   "source": [
    "This blocks also can be used independently, depending on the task:\n",
    "\n",
    "- **Encoder-only models:** Good for tasks that require understaind og the inputs, such as: sentence classification and named entity recognition.\n",
    "- **Decoder-only models:** Good for generative tasks such as text generation.\n",
    "- **Encoder-decoder models** or **Sequence-to-sequence models:** Good for genereative tasks that require an input such as translation or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514112a",
   "metadata": {},
   "source": [
    "## Attention Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b60d57",
   "metadata": {},
   "source": [
    "A key feature of Transformer models is that they are built with special layers called attention layers. This layer will tell the model specific words to pay more attention, giving less attention to the other words.\n",
    "\n",
    "This helps the model to deal with different contexts and grammatical specificities such as noun and verbs order in the sentence(in different languages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda03fc",
   "metadata": {},
   "source": [
    "# The Original Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf8179",
   "metadata": {},
   "source": [
    "The transforme architecture was originally designed for translation. During training, the encoder receives inputs(sentences) in a certain language, while the decoder receives the same sentences in the desired target language:\n",
    "\n",
    "1. In the encoder, the attention layers can use all the words in a sentence(the translation of a given word can be dependent on what is after as well as before it in sentence)\n",
    "2. The decoder, works sequentially and can only pay attention to the words in the sentence that it has already translated(only the words before the word currently being generated).\n",
    "3. The decoder works in two ways: In training the decoder is fed with the whole target(translated sequence) but with masks on the word it has to predict. In the sequentially way, the decoder predict each of the words sequentially based on the encoder output and in it last predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938bacc",
   "metadata": {},
   "source": [
    "Original Transformer architecture:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94318ecb",
   "metadata": {},
   "source": [
    "![image.png](images/image%205.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe406f",
   "metadata": {},
   "source": [
    "**Architecture vs Checkpoints**\n",
    "\n",
    "**Architecture:** The definition of each layer and each operation that happens within the model\n",
    "\n",
    "**Checkpoints:** The wheights that will be loaded in a given architecture"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
