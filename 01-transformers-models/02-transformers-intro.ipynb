{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f1d2e2",
   "metadata": {},
   "source": [
    "# Transformers, what can they do?\n",
    "\n",
    "- The first tool from transformers library: **`pipeline()`** function\n",
    "\n",
    "Transformers library is a open source library from hugging face that provides access to a variety of LLMs models, from different companies.\n",
    "\n",
    "The Model Hub contains millions of pretained models that anyone can download and use.\n",
    "\n",
    "# Working with pipelines\n",
    "\n",
    "The basic object in the transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us ot direct input any text and get an intelligible answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1788b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9516065716743469}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053158d1",
   "metadata": {},
   "source": [
    "We can pass several sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea689c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9516065716743469},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995144605636597}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I've been waiting for a HuggingFace course my whole life\", \"I hate this so much\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53b281",
   "metadata": {},
   "source": [
    "This pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create a classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbd2a4",
   "metadata": {},
   "source": [
    "\n",
    "**Encoder - Decoder Architecture:**\n",
    "There are three main steps involved when you pass some text to a pipeline:\n",
    "\n",
    "1. The text is preprocessed into a format the model can understand(Tokenization - Embedding)\n",
    "2. The preprocessed inputs are passed to the model\n",
    "3. The predictions of the model are post-processed, so you can make sense of them\n",
    "\n",
    "The `pipeline()` function supports multiple modalities. Such as:\n",
    "\n",
    "# Text pipelines\n",
    "- **text-generation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57e8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to create the necessary HTML with JavaScript.\\n\\nPrerequisites\\n\\nYou must be able to understand JavaScript and HTML. Your experience with any language and its code can only get better as you learn more and'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ceb1f",
   "metadata": {},
   "source": [
    "You can also use parameters such as `num_return_sequences` to control how many different sequences are generated and `max_length` to control the length of the output text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36203fb0",
   "metadata": {},
   "source": [
    "- **summarization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6828f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the number of graduates in traditional engineering disciplines has declined . in most of the premier american universities engineering curricula now concentrate on and encourage largely the study of engineering science . rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75930b97",
   "metadata": {},
   "source": [
    "you can specify a `max_length` or a `min_length` for the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66708fba",
   "metadata": {},
   "source": [
    "- **translation:**\n",
    "you can use a specific model to translate from a language to another, example, from french to english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd1fa93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-fr-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0c5374005f47979d2aab0741413faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ezequ\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6297eb4b41f400a801e599ed6d9c490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fee93527e34f5d8d220d98a949b50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb5ad9",
   "metadata": {},
   "source": [
    "- **zero-shot-classification:** Classify text without prior training on specific labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e6c9e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to FacebookAI/roberta-large-mnli and revision 2a8f12d (https://huggingface.co/FacebookAI/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96be40d33044263999bfffc71cc3de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ezequ\\.cache\\huggingface\\hub\\models--FacebookAI--roberta-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61a87ee50c44601b7a1e271b918fca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0a63373d59436d96850419148def6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6622f29ad84db490639fc3ce70dc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc164f0995d41339fda8e6cc8a2f102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2d38a650cb4e86832122e88a03757a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9562343955039978, 0.026972239837050438, 0.01679336093366146]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "\t\"This is a course about the Transformers library\", \n",
    "\tcandidate_labels = [\"education\", \"politics\", \"business\"],\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2ce40",
   "metadata": {},
   "source": [
    "- **fill-mask:** Fill in the blanks in a given text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1766b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f66392fb934cd3a9a9815f61225a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ezequ\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea15c0e9b9146dfa5d86e78cc458bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c8150adb374705bbb0326a157dba55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259855703f7e4655a09b654b17d3ce3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdad83fb72954ef5ba8149ae13f02400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae328edd63f04875a4028575346c45a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19631439447402954,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models'},\n",
       " {'score': 0.04449189826846123,\n",
       "  'token': 745,\n",
       "  'token_str': ' building',\n",
       "  'sequence': 'This course will teach you all about building models'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b55bbd",
   "metadata": {},
   "source": [
    "`top_k` is the argument that controls how many possibilities you want to be displayed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801a287",
   "metadata": {},
   "source": [
    "- **named-entity-recognition(ner)**: Is a task where the model has to find which parts of the input text correspond to entities such as persons, locations or organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56420c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff7afd995b54e248b8ee0acb947b215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ezequ\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 1334.40 MB. The target location C:\\Users\\ezequ\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english\\blobs only has 1120.31 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86655a83cfd9412ea332723b16b0711c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 1334.40 MB. The target location C:\\Users\\ezequ\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english\\blobs only has 447.78 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63812e679149492bae0bdbbfacefe4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model dbmdz/bert-large-cased-finetuned-conll03-english with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForTokenClassification'>, <class 'transformers.models.bert.modeling_tf_bert.TFBertForTokenClassification'>). See the original errors:\n\nwhile loading with TFAutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2815, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 312, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 557, in cached_files\n    raise e\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 470, in cached_files\n    hf_hub_download(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1008, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1161, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1710, in _download_to_tmp_and_move\n    xet_get(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 627, in xet_get\n    download_files(\nRuntimeError: Data processing error: CAS service error : IO Error: There is not enough space on the disk. (os error 112)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2871, in from_pretrained\n    raise EnvironmentError(\nOSError: Can't load the model for 'dbmdz/bert-large-cased-finetuned-conll03-english'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dbmdz/bert-large-cased-finetuned-conll03-english' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\nwhile loading with TFBertForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2815, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 312, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 557, in cached_files\n    raise e\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 470, in cached_files\n    hf_hub_download(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1008, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1161, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1710, in _download_to_tmp_and_move\n    xet_get(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 627, in xet_get\n    download_files(\nRuntimeError: Data processing error: CAS service error : IO Error: There is not enough space on the disk. (os error 112)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2871, in from_pretrained\n    raise EnvironmentError(\nOSError: Can't load the model for 'dbmdz/bert-large-cased-finetuned-conll03-english'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dbmdz/bert-large-cased-finetuned-conll03-english' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ner \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouped_entities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m ner(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy name is Sylvain and I work at Hugging Face in Brooklyn;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 942\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    943\u001b[0m         adapter_path \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[0;32m    944\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    945\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    946\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    947\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    950\u001b[0m     )\n\u001b[0;32m    952\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    953\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:305\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    304\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model dbmdz/bert-large-cased-finetuned-conll03-english with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForTokenClassification'>, <class 'transformers.models.bert.modeling_tf_bert.TFBertForTokenClassification'>). See the original errors:\n\nwhile loading with TFAutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2815, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 312, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 557, in cached_files\n    raise e\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 470, in cached_files\n    hf_hub_download(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1008, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1161, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1710, in _download_to_tmp_and_move\n    xet_get(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 627, in xet_get\n    download_files(\nRuntimeError: Data processing error: CAS service error : IO Error: There is not enough space on the disk. (os error 112)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2871, in from_pretrained\n    raise EnvironmentError(\nOSError: Can't load the model for 'dbmdz/bert-large-cased-finetuned-conll03-english'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dbmdz/bert-large-cased-finetuned-conll03-english' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\nwhile loading with TFBertForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2815, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 312, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 557, in cached_files\n    raise e\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 470, in cached_files\n    hf_hub_download(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1008, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1161, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1710, in _download_to_tmp_and_move\n    xet_get(\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 627, in xet_get\n    download_files(\nRuntimeError: Data processing error: CAS service error : IO Error: There is not enough space on the disk. (os error 112)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2871, in from_pretrained\n    raise EnvironmentError(\nOSError: Can't load the model for 'dbmdz/bert-large-cased-finetuned-conll03-english'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'dbmdz/bert-large-cased-finetuned-conll03-english' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\n\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf236b",
   "metadata": {},
   "source": [
    "Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC). The parameter grouped_entities is used to thell the pipeline to regroup together the parts of the sentence that correspond to the same entity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5983ce9b",
   "metadata": {},
   "source": [
    "- **question_answering:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabd146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1410820ab604ae2b3cca258cf4e7acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezequ\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ezequ\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12f571e1f5a4ba7a538b0df1e52fed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50b4d24587d427499aa77bbcedb8657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817a8b353e914b4697ed302e95c09664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23b256a8b7d4738a73645a2b094aa96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949754953384399, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "\tquestion=\"Where do I work?\",\n",
    "\tcontext=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    "\t)\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed15fe",
   "metadata": {},
   "source": [
    "This pipeline works by extracting information from the provided context, it does not generate the answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c50f8e",
   "metadata": {},
   "source": [
    "# Image pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b42f5",
   "metadata": {},
   "source": [
    "- **image-classification:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c68c4c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7233c485734b72a2a37959f6638470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3034d1889d9443d3af306c6fe8e30843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df51b5df5464fbaa626ce2721c9ebbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_classifier = pipeline(task=\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "result = image_classifier(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ac484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'lynx, catamount', 'score': 0.4403035044670105}, {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'score': 0.03433411568403244}, {'label': 'snow leopard, ounce, Panthera uncia', 'score': 0.03214793652296066}, {'label': 'Egyptian cat', 'score': 0.023539062589406967}, {'label': 'tiger cat', 'score': 0.023034056648612022}]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc10481",
   "metadata": {},
   "source": [
    "There are other image pipeline use-cases, such as:\n",
    "- **object-detection**\n",
    "- **image-to-text**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7e278",
   "metadata": {},
   "source": [
    "# Audio pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150f2d4",
   "metadata": {},
   "source": [
    "- **automatic-speech-recognition:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFWhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of TFWhisperForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWhisperForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "transcriber = pipeline(\n",
    "\ttask=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\"\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ce7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = transcriber(\n",
    "    \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454477c",
   "metadata": {},
   "source": [
    "There are other image pipeline use-cases, such as:\n",
    "- **audio-classification**\n",
    "- **text-to-speech**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d33d29",
   "metadata": {},
   "source": [
    "# Multimodal pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc561df",
   "metadata": {},
   "source": [
    "It’s possible to use a particular model from the Hub to use in a pipeline for a specific task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTb/SmolLM2-360M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdabb6e",
   "metadata": {},
   "source": [
    "So, we can use specific models, such as, models trained in different languages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
