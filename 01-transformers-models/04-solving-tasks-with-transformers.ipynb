{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1cb77faf",
      "metadata": {
        "id": "1cb77faf"
      },
      "source": [
        "# Solving Taks With Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers models for language"
      ],
      "metadata": {
        "id": "PPSzkf_O8d6r"
      },
      "id": "PPSzkf_O8d6r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language models are at the heart of modern NLP. The transformers was originally built for machine translation and then, it has become the default architecture for solving AI tasks. Some tasks are more suitable for transformer's encoder structure while others, for the decoder. Still, other tasks make use of both the Transformer's encoder-decoder structure."
      ],
      "metadata": {
        "id": "cCQiuOd88u2W"
      },
      "id": "cCQiuOd88u2W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How language models work"
      ],
      "metadata": {
        "id": "nBl1kIRh9aup"
      },
      "id": "nBl1kIRh9aup"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language models work by being trained to predict the probabilty of a word given the context of surrounding words. This gives them a foundational understanding of language that can generalize to other tasks.\n",
        "There are two main approaches fro training a transformer model:"
      ],
      "metadata": {
        "id": "-njkftJp9pCt"
      },
      "id": "-njkftJp9pCt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Masked language modeling(MLM): Used by encoder models like BERT, the input tokens are randomly masked, then the model is trained to predict the original tokens based on the surrounding context. The context is bidirectional, the model learns based on the words both before and after the masked word.  \n",
        "- Causal language modeling(CLM): Used by decoder models like GPT, this approach predicts the next token based on all previous tokens in the sequence. The context is unidirectional, the model learns based only on context from the left(previous tokens)."
      ],
      "metadata": {
        "id": "JRdN9APv9-1p"
      },
      "id": "JRdN9APv9-1p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of language models"
      ],
      "metadata": {
        "id": "GLw8ymSTBqYI"
      },
      "id": "GLw8ymSTBqYI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encoder-only-models: Uses a bidirectional approach to understand context from both directions. Best suited for tasks that require deep understanding of text, such as classification, named entity recognition, and question answering.\n",
        "- Decoder-only-models: Process text from left to rigt and are good at text generation tasks. They can complete sentences, write essays or even generate code based on a prompt.\n",
        "- Encoder-decoder-models: Combine both approaches, suing an encoder to understand the input and a decoder to generate output. They're usefull for tasks like translation, summarization and question answering.\n",
        "\n"
      ],
      "metadata": {
        "id": "-B54gBjMB1-M"
      },
      "id": "-B54gBjMB1-M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some tasks and which type of model usually perform it:\n",
        "- **Text Generation:** Decoder only, like GP2.\n",
        "\n",
        "- **Text Classification:** Encoder Only, like BERT with a classification head on top of the base model.\n",
        "\n",
        "- **Token Classification:**  BERT with a token classification head on top of the base model.\n",
        "\n",
        "- **Question Answering:** BERT with a span classification head on top of the base model.\n",
        "\n",
        "- **Summarization:**  Encoder-Decoder models like BART.\n",
        "\n",
        "- **Translation:**  Encoder-Decoder models like BART."
      ],
      "metadata": {
        "id": "vRxvPkwrFCgX"
      },
      "id": "vRxvPkwrFCgX"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}