{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3af01fe",
   "metadata": {},
   "source": [
    "# Inference with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42e033",
   "metadata": {},
   "source": [
    "Inference is the process of using a trianed LLM to generate human-like text from a given input prompt. Language models use their knowledge from training to formulate responses one word at time. The model leverages learned probabilites from bilions of parameters to predict and generate the next token in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431333c3",
   "metadata": {},
   "source": [
    "Attention mechanism is the key to LLMs understand context and generate coherent text based on it. When predicting the next word, not every word in a sentence has equal weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89602d9",
   "metadata": {},
   "source": [
    "## Context Length and Attention Span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f16a1",
   "metadata": {},
   "source": [
    "Context length is the maximum number of tokens that the LLM can process at once. i.e. The size of the model's working memory.\n",
    "These capabilities are limited by factors such as:\n",
    "- Model's architecture and size\n",
    "- Available computational resources\n",
    "- The complexity of the input and desired output\n",
    "In an ideal world, we could feed unilimted context to the model, but hardware constraints and computational costs make this impractical. This is why different models are designed with different context lengths to balance capability with efficiency.\n",
    "\n",
    "Essentially, is the maximum number of tokens that a model considers when generating a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f175a6d",
   "metadata": {},
   "source": [
    "## The Two-Phase Inference Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c27a8",
   "metadata": {},
   "source": [
    "The text generating process can be brokend own into two main phases: prefill and decode. These phases work together, each playing a crucial role in producing coherent text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38bc59",
   "metadata": {},
   "source": [
    "* **Prefill Phase**\n",
    "    The preparation stage, this phase involves three key steps:\n",
    "    1. **Tokenization**: Convert the input text into tokens.\n",
    "    2. **Embedding Conversinon**: Transforming these tokens into numerical representations that capture their meaning.\n",
    "    3. **Initial Processing**: Running these embeddings through the model's neural network to create a rich understanding of the context. \n",
    "\n",
    "* **Decode Phase**\n",
    "    Where the actual text generation happens:\n",
    "    1. **Attention Computation**: Looking back at all previous tokens to understand context.\n",
    "    2. **Probabilty Calculation**: Determining the likelihood of each possible next token.\n",
    "    3. **Token Selection**: Choosing the next token based on these probabilities.\n",
    "    4. **Continuation Check**: Deciding whether to continue or stop generation.\n",
    "    This is memory-intensive because the model needs to keep track of all previously generated tokens and their relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3af3d",
   "metadata": {},
   "source": [
    "## Sampling Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40605f4",
   "metadata": {},
   "source": [
    "Let's explore the various ways we can control this generation process. This is the moment where the model chooses between being more creative or more precise, the token selection process is adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8b5c4",
   "metadata": {},
   "source": [
    "**Token Selection**: When the moodel need to chose the next token, it starts with raw probabilities(logits) for every word in its vocabulary. But how we turn thse probabilities into actual choices? This is the process:\n",
    "1. **Raw Logits:** Think of these as the model's initial gut feelings about each possible next word.\n",
    "2. **Temperature Control:** Creativity measurement, from lower settings(<1) to make more focused and deterministic to higher settings(>1) to make choices more random and creative.\n",
    "3. **Top-p sampling**: Instead of considering all possible words, look at the most likely ones that add up to a chosen probability threshold\n",
    "4. **Top-k filtering:** Alternatvia approach, consider the k most likely words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28338029",
   "metadata": {},
   "source": [
    "## Managing Repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae4d45",
   "metadata": {},
   "source": [
    "One common behavior to LLMs is to repeat themselves, so it's used two penalties to prevent this:\n",
    "- **Presence Penalty**: A fixed penalty applied to any token that has appeared before, regardless of how often. \n",
    "- **Frequency Penalty**: A scaling penalty that increaes based on how often a token has been used. The more a word appears, the less likely it is to be chosen again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd4ae7",
   "metadata": {},
   "source": [
    "## Controlling generation length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcbc93",
   "metadata": {},
   "source": [
    "It's important to control how much text the LLM generates. This length is controlled using some methods as:\n",
    "- **Token Limits:** Setting minimum and maximum token counts.\n",
    "- **Stop Sequences:** Defining specific patterns that signal the end of generation.\n",
    "- **End-of-Sequence Detection:** Letting the model naturally conclude its response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a385fd4",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b485cd2",
   "metadata": {},
   "source": [
    "Beam search it's a multi responses strategie, where the model generate multiple possible paths of generated responses and then evaluate the best:\n",
    "1. At each step, maintain multiple candidate sequences\n",
    "2. For each candidate, compute probabilities for the next token\n",
    "3. Keep only the most promising combinatins of sequences and next tokens\n",
    "4. Continue this process until reaching a stop condition\n",
    "5. Select the sequence with the highest overall probability\n",
    "\n",
    "This approach often produces more coherent and grammatically correct text, but it requires more computational resources than simpler methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb19b8f",
   "metadata": {},
   "source": [
    "## Key Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9086be",
   "metadata": {},
   "source": [
    "When working with LLMs, frou critical metrics will shape your implementation decisions:\n",
    "1. **Time to First Token(TTFT):** How quickly can you get the first response?\n",
    "2. **Time Per Output Token (TPOT):** How fast can you generate the subsequent tokens?\n",
    "3. **Throughput:** How many requests can you handle simultaneously?\n",
    "4. **VRAM Usage:** How much GPU memory do you need?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22349414",
   "metadata": {},
   "source": [
    "## The Context Length Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbd61a",
   "metadata": {},
   "source": [
    "One of the most significant challenges in LLM inference is managing context length effectively. Longer contexts provide more information but come with substantial costs:\n",
    "- **Memory usage:** Grows **quadratically** with context length\n",
    "- **Processing speed:** Decreases **linearly** with longer contexts\n",
    "- **Resource Allocation:** Requires careful balancing of VRAM usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67a32f",
   "metadata": {},
   "source": [
    "### The KV Cache Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d8a57",
   "metadata": {},
   "source": [
    "To address these challenges, one powerfull optimization is KV(Key-Value) caching, is to storage the key-value matrix for each token during inference in a cache memory. This technique significantly improves inference speed by storing and reusing intermediate calculations. This optimization:\n",
    "- Reduces repeated calculations\n",
    "- Improves generation speed\n",
    "- Makes long-context generation practical\n",
    "\n",
    "The trade-off is additional memory usage, but the performance benefits usually far outweigh this cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bd94d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
